from airflow import DAG
from airflow.providers.amazon.aws.hooks.s3 import S3Hook
from airflow.operators.python import PythonOperator
from airflow.providers.amazon.aws.sensors.s3 import S3KeySensor
from airflow.providers.apache.spark.operators.spark_submit import SparkSubmitOperator

import random
import time
from io import StringIO, BytesIO
from datetime import datetime, timedelta
from zoneinfo import ZoneInfo

# PyArrow import for Parquet
import pyarrow as pa
import pyarrow.parquet as pq

from kafka import KafkaAdminClient
from kafka.errors import KafkaError

# Confluent Kafka ë° Avro ê´€ë ¨ import
from confluent_kafka import Producer as ConfluentProducer, Consumer as ConfluentConsumer, KafkaError as ConfluentKafkaError, TopicPartition
from confluent_kafka.avro import AvroProducer, AvroConsumer
from confluent_kafka.avro.serializer import SerializerError
from confluent_kafka.schema_registry import SchemaRegistryClient
from confluent_kafka.admin import AdminClient as ConfluentAdminClient # AdminClient ì„í¬íŠ¸ ì¶”ê°€
from confluent_kafka.schema_registry.avro import AvroSerializer, AvroDeserializer # ì§ì ‘ ì‚¬ìš©í•  ê²½ìš° í•„ìš”


from utils.slack_fail_noti import task_fail_slack_alert

KAFKA_TOPIC_AVRO = 'userlog-avro-topic' # Avro ë©”ì‹œì§€ë¥¼ ìœ„í•œ ìƒˆ í† í”½

kafka_cluster = '43.201.43.88:9092,15.165.234.219:9092,3.35.228.177:9092'


def connect_minio():
    s3_hook = S3Hook(
        aws_conn_id='minio',
        region_name='us-east-1' # MinIOëŠ” region ê°œë…ì´ ì—†ì§€ë§Œ, S3Hookì€ í•„ìš”ë¡œ í•¨
    )
    return s3_hook

def kafka_consumer(**context):
    consumer = None
    processed_messages_in_session = 0 # ì´ë²ˆ íƒœìŠ¤í¬ ì‹¤í–‰ì—ì„œ ì²˜ë¦¬ëœ ì´ ë©”ì‹œì§€ ìˆ˜
    # ê° íŒŒí‹°ì…˜ë³„ë¡œ ì»¤ë°‹í•  ì˜¤í”„ì…‹ì„ ì €ì¥í•˜ëŠ” ë”•ì…”ë„ˆë¦¬
    # key: TopicPartition(topic, partition), value: offset (ì»¤ë°‹í•  ë‹¤ìŒ ë©”ì‹œì§€ì˜ ì˜¤í”„ì…‹)
    offsets_to_commit_map = {}

    try:
        consumer_config = {
            'bootstrap.servers': kafka_cluster, # ì‹¤ì œ Kafka ë¸Œë¡œì»¤ ì£¼ì†Œ
            'group.id': 'avro-userlog-consumer-group-01', # ì»¨ìŠˆë¨¸ ê·¸ë£¹ ID
            'schema.registry.url': SCHEMA_REGISTRY_URL,
            'auto.offset.reset': 'earliest',
            'enable.auto.commit': False, # ìˆ˜ë™ ì»¤ë°‹
            # 'fetch.min.bytes': 1, # ê¸°ë³¸ê°’
            # 'fetch.max.wait.ms': 500, # ê¸°ë³¸ê°’
            # 'max.poll.records': 500, # confluent-kafkaì—ì„œëŠ” ì´ ì„¤ì •ì´ ì§ì ‘ì ìœ¼ë¡œ poll() ë°˜í™˜ ê°œìˆ˜ë¥¼ ì œì–´í•˜ì§€ ì•ŠìŒ
                                     # poll()ì€ í•œ ë²ˆì— ì—¬ëŸ¬ ë©”ì‹œì§€ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ìˆì§€ë§Œ, ë°˜í™˜ì€ í•˜ë‚˜ì”© ë˜ëŠ” ì—ëŸ¬/íƒ€ì„ì•„ì›ƒ
        }
        # AvroConsumerëŠ” value_schemaë¥¼ ëª…ì‹œì ìœ¼ë¡œ ì „ë‹¬í•  í•„ìš” ì—†ìŒ (Schema Registryì—ì„œ ê°€ì ¸ì˜´)
        consumer = AvroConsumer(consumer_config)
        consumer.subscribe([KAFKA_TOPIC_AVRO])
        print(f"âœ… AvroConsumer subscribed to topic '{KAFKA_TOPIC_AVRO}' with group ID '{consumer_config['group.id']}'")

        last_message_received_time = time.time()
        POLL_TIMEOUT_S = 1.0  # poll íƒ€ì„ì•„ì›ƒ (ì´ˆ ë‹¨ìœ„)
        IDLE_CONSUMPTION_TIMEOUT_S = 30 # ë§ˆì§€ë§‰ ë©”ì‹œì§€ ìˆ˜ì‹  í›„ ì´ ì‹œê°„ ë™ì•ˆ ë©”ì‹œì§€ ì—†ìœ¼ë©´ ì¢…ë£Œ (ì´ˆ)
        MAX_COLLECTION_DURATION_S = 300 # ìµœëŒ€ ì‹¤í–‰ ì‹œê°„ (ì´ˆ)
        collection_start_time = time.time()

        print(f"ğŸš€ Starting message collection for up to {MAX_COLLECTION_DURATION_S}s or until idle for {IDLE_CONSUMPTION_TIMEOUT_S}s.")

        s3_hook = connect_minio()
        print("âœ… MinIO connected")
        bucket_name = "userlog-data" # MinIO ë²„í‚· ì´ë¦„
        if not s3_hook.check_for_bucket(bucket_name):
            print(f"Bucket '{bucket_name}' does not exist. Creating...")
            s3_hook.create_bucket(bucket_name=bucket_name)
            print(f"âœ… Bucket '{bucket_name}' created.")
        else:
            print(f"âœ… Bucket '{bucket_name}' already exists.")

        current_time_kst = datetime.now(ZoneInfo("Asia/Seoul"))
        base_filename = current_time_kst.strftime("%Y-%m-%d_%H-%M-%S")
         # íŒŒì¼ëª…ì„ Parquetìœ¼ë¡œ ë³€ê²½í•˜ê³  S3 ê²½ë¡œë„ ì—…ë°ì´íŠ¸
        parquet_filename = f"{base_filename}_avro_consumed.parquet"
        s3_parquet_object_key = f"test/user-activity-raw-parquet/{parquet_filename}" # S3 ê²½ë¡œë„ Parquetìœ¼ë¡œ ëª…ì‹œ

        collected_events = [] # ì—­ì§ë ¬í™”ëœ ì´ë²¤íŠ¸ë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸

         # ë¡œì»¬ JSON íŒŒì¼ ìƒì„± ë¡œì§ ëŒ€ì‹ , ë¦¬ìŠ¤íŠ¸ì— ë©”ì‹œì§€ ìˆ˜ì§‘
        # with open(local_json_filename, "w") as f: # ì´ ë¶€ë¶„ ì œê±°
        while True:
            # ìµœëŒ€ ì‹¤í–‰ ì‹œê°„ ì´ˆê³¼ í™•ì¸
            if time.time() - collection_start_time > MAX_COLLECTION_DURATION_S:
                print(f"â„¹ï¸ Max collection duration of {MAX_COLLECTION_DURATION_S}s reached.")
                break

            msg = consumer.poll(timeout=POLL_TIMEOUT_S)

            if msg is None: # íƒ€ì„ì•„ì›ƒ, ë©”ì‹œì§€ ì—†ìŒ
                if time.time() - last_message_received_time > IDLE_CONSUMPTION_TIMEOUT_S:
                    print(f"â„¹ï¸ No messages received for {IDLE_CONSUMPTION_TIMEOUT_S}s. Finalizing batch.")
                    break
                continue # ì•„ì§ idle íƒ€ì„ì•„ì›ƒì— ë„ë‹¬í•˜ì§€ ì•Šì•˜ìœ¼ë©´ ê³„ì† í´ë§

            if msg.error():
                if msg.error().code() == ConfluentKafkaError._PARTITION_EOF:
                    print(f"â„¹ï¸ Reached end of partition for {msg.topic()} [{msg.partition()}] at offset {msg.offset()}")
                    continue
                elif msg.error().code() == ConfluentKafkaError.UNKNOWN_TOPIC_OR_PART:
                    print(f"âŒ Error: Unknown topic or partition: {msg.error()}. Waiting for metadata refresh...")
                    time.sleep(5) # ì ì‹œ ëŒ€ê¸° í›„ ì¬ì‹œë„ (ë©”íƒ€ë°ì´í„° ê°±ì‹  ì‹œê°„ ë¶€ì—¬)
                    continue
                else:
                    print(f"âŒ Consumer error: {msg.error()}")
                    raise Exception(f"Consumer error: {msg.error()}")

            last_message_received_time = time.time() # ë©”ì‹œì§€ ìˆ˜ì‹  ì‹œê°„ ê°±ì‹ 
            try:
                event_data = msg.value() # Avro ì—­ì§ë ¬í™”ëœ Python dict
                if event_data is None: # Tombstone ë©”ì‹œì§€ ë“± valueê°€ nullì¸ ê²½ìš°
                    print(f"â„¹ï¸ Received a message with null value (possibly tombstone) at {msg.topic()} [{msg.partition()}] offset {msg.offset()}. Skipping.")
                    tp_key = (msg.topic(), msg.partition())
                    offsets_to_commit_map[tp_key] = msg.offset() + 1
                    continue

                # JSON íŒŒì¼ì— ì“°ëŠ” ëŒ€ì‹  ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
                collected_events.append(event_data)
                processed_messages_in_session += 1

                # ì»¤ë°‹í•  ì˜¤í”„ì…‹ ì—…ë°ì´íŠ¸ (íŒŒí‹°ì…˜ë³„ë¡œ ë‹¤ìŒ ë©”ì‹œì§€ì˜ ì˜¤í”„ì…‹)
                tp_key = (msg.topic(), msg.partition())
                offsets_to_commit_map[tp_key] = msg.offset() + 1

                if processed_messages_in_session % 100 == 0: # ì˜ˆ: 100ê°œ ë©”ì‹œì§€ ì²˜ë¦¬ë§ˆë‹¤ ë¡œê·¸ ì¶œë ¥
                    print(f"ğŸ“ Processed {processed_messages_in_session} messages so far in this session...")

            except SerializerError as e:
                print(f"âŒ Message deserialization failed for message at offset {msg.offset()}: {e}. Skipping.")
                tp_key = (msg.topic(), msg.partition())
                offsets_to_commit_map[tp_key] = msg.offset() + 1
                continue # ë‹¤ìŒ ë©”ì‹œì§€ ì²˜ë¦¬
            except Exception as e:
                print(f"âŒ Failed to process message (value: {msg.value() if msg else 'N/A'}) for writing: {e}")
                raise e

        if processed_messages_in_session == 0:
            print("â„¹ï¸ No messages processed during the collection period.")
            # ë¡œì»¬ íŒŒì¼ ê´€ë ¨ ë¡œì§ ì œê±°
            print("âœ… No messages to process. Task finished gracefully.")
            context['ti'].xcom_push(key='s3_object_key', value=None) # XCom í‚¤ í†µì¼
            context['ti'].xcom_push(key='base_filename_for_processed', value=None)
            return # ì •ìƒ ì¢…ë£Œ

        print(f"âœ… Collected {processed_messages_in_session} messages. Converting to Parquet and uploading to MinIO.")

        # ìˆ˜ì§‘ëœ ì´ë²¤íŠ¸ë¥¼ PyArrow Tableë¡œ ë³€í™˜ í›„ Parquetìœ¼ë¡œ MinIOì— ì—…ë¡œë“œ
        if collected_events:
            try:
                arrow_table = pa.Table.from_pylist(collected_events)
                
                parquet_buffer = BytesIO()
                pq.write_table(arrow_table, parquet_buffer)
                parquet_buffer.seek(0)

                s3_hook.load_file_obj(parquet_buffer, s3_parquet_object_key, bucket_name, replace=True)
                print(f"âœ… Parquet file {parquet_filename} uploaded to MinIO: s3://{bucket_name}/{s3_parquet_object_key}")
                
                # XCom PUSH (Parquet ê²½ë¡œ)
                context['ti'].xcom_push(key='s3_object_key', value=s3_parquet_object_key) # XCom í‚¤ í†µì¼
                context['ti'].xcom_push(key='base_filename_for_processed', value=base_filename)

            except Exception as e:
                print(f"âŒ Failed to convert to Parquet or upload to MinIO: {e}")
                # Parquet ë³€í™˜/ì—…ë¡œë“œ ì‹¤íŒ¨ ì‹œ XCom ê°’ ì„¤ì • ë°©ì§€ ë˜ëŠ” ì—ëŸ¬ì— ë”°ë¥¸ ì²˜ë¦¬
                context['ti'].xcom_push(key='s3_object_key', value=None)
                context['ti'].xcom_push(key='base_filename_for_processed', value=None)
                raise e
        else: # processed_messages_in_session > 0 ì´ì§€ë§Œ collected_eventsê°€ ë¹„ì–´ìˆëŠ” ê²½ìš°ëŠ” ê±°ì˜ ì—†ìœ¼ë‚˜ ë°©ì–´ì ìœ¼ë¡œ ì²˜ë¦¬
            print("â„¹ï¸ No events in collected_events list, skipping Parquet conversion and upload.")
            context['ti'].xcom_push(key='s3_object_key', value=None)
            context['ti'].xcom_push(key='base_filename_for_processed', value=None)


        # ì˜¤í”„ì…‹ ì»¤ë°‹ (ì´ ë¶€ë¶„ì€ ë³€ê²½ ì—†ìŒ)
        if offsets_to_commit_map:
            commit_list = [TopicPartition(topic, partition, offset) for (topic, partition), offset in offsets_to_commit_map.items()]
            try:
                consumer.commit(offsets=commit_list, asynchronous=False) # ë™ê¸° ì»¤ë°‹
                print(f"âœ… Successfully committed offsets for {len(commit_list)} partitions.")
                for tp_offset in commit_list:
                    print(f"  - Topic: {tp_offset.topic}, Partition: {tp_offset.partition}, Offset: {tp_offset.offset}")
            except Exception as e:
                print(f"âŒ Failed to commit offsets: {e}")
                raise e
        else:
            print("â„¹ï¸ No offsets to commit.")

        # XCom PUSH ë¡œì§ì€ Parquet ì—…ë¡œë“œ ì„±ê³µ ì‹œ ì´ë¯¸ ìˆ˜í–‰ë¨

    except Exception as e:
        print(f"âŒ Kafka consumer task failed: {e}")
        # ì‹¤íŒ¨ ì‹œì—ë„ XComì„ Noneìœ¼ë¡œ ì„¤ì •í•˜ì—¬ í›„ì† ì‘ì—…ì´ ì˜¤ë™ì‘í•˜ì§€ ì•Šë„ë¡ í•  ìˆ˜ ìˆìŒ
        context['ti'].xcom_push(key='s3_object_key', value=None)
        context['ti'].xcom_push(key='base_filename_for_processed', value=None)
        raise e
    finally:
        if consumer:
            print("â„¹ï¸ Closing Kafka consumer.")
            consumer.close()
        # ë¡œì»¬ íŒŒì¼ ìƒì„±/ì‚­ì œ ë¡œì§ì´ ì—†ì–´ì¡Œìœ¼ë¯€ë¡œ ê´€ë ¨ ì½”ë“œ ì œê±°


def check_kafka_broker_health():
    brokers = kafka_cluster.split(',') # Split the comma-separated string into a list of brokers
    alive_count = 0
    admin_client = None # finally ë¸”ë¡ì—ì„œ ì‚¬ìš©í•˜ê¸° ìœ„í•´ ì´ˆê¸°í™”

    for broker_url in brokers:
        try:
            admin_client = ConfluentAdminClient( # ConfluentAdminClient ì‚¬ìš©
                bootstrap_servers=broker_url,
                client_id='kafka-health-check',
                request_timeout_ms=5000
            )
            topics = admin_client.list_topics() # Removed timeout_ms argument
            print(f"âœ… Broker {broker_url} is alive. Found {len(topics)} topics.")
            alive_count += 1
        except ConfluentKafkaError as e: # ConfluentKafkaError ì‚¬ìš©
            print(f"âŒ Broker {broker_url} health check failed: {e}")
        except Exception as e:
            print(f"âŒ An unexpected error occurred while checking broker {broker_url}: {e}")
        finally:
            if admin_client:
                try:
                    admin_client.close()
                except Exception as e_close:
                    print(f"âš ï¸ Error closing AdminClient for {broker_url}: {e_close}")
                admin_client = None # ë‹¤ìŒ ë£¨í”„ë¥¼ ìœ„í•´ ì´ˆê¸°í™”

    MIN_ALIVE_BROKERS = 2
    if alive_count < MIN_ALIVE_BROKERS:
        raise Exception(f"Kafka cluster health check failed: Only {alive_count} out of {len(brokers)} brokers are alive. Required: {MIN_ALIVE_BROKERS}.")
    else:
        print(f"âœ… Kafka cluster health check passed: {alive_count} brokers are alive.")


with DAG(
    dag_id='avro_userlog_pipeline', 
    default_args={
        'owner': 'airflow',
        'depends_on_past': False,
        'email_on_failure': False,
        'email_on_retry': False,
        'retries': 2, 
        'retry_delay': timedelta(minutes=1), 
        'execution_timeout': timedelta(minutes=120), 
        'on_failure_callback': task_fail_slack_alert, 
    },
    description="Userlog pipeline: Kafka(Avro) -> MinIO(Parquet) -> Iceberg", # ì„¤ëª… ì—…ë°ì´íŠ¸
    start_date=datetime(2025, 6, 1), 
    catchup=False, 
    schedule_interval='*/30 * * * *', # 30ë¶„ ê°„ê²©ìœ¼ë¡œ ì‹¤í–‰
    tags=['userlog', 'avro', 'kafka', 'parquet', 'minio', 'iceberg'] # íƒœê·¸ ì—…ë°ì´íŠ¸
) as dag:

    check_kafka_brokers_health = PythonOperator(
        task_id='check_kafka_broker_health',
        python_callable=check_kafka_broker_health,
    )

    consume_avro_data_to_minio = PythonOperator(
        task_id='kafka_consumer_avro_to_parquet_minio', # íƒœìŠ¤í¬ ID ë³€ê²½
        python_callable=kafka_consumer,
    )

    check_minio_file_upload = S3KeySensor(
        task_id='check_minio_file_upload',
        bucket_name='userlog-data', 
        bucket_key="{{ ti.xcom_pull(task_ids='kafka_consumer_avro_to_parquet_minio', key='s3_object_key') }}", # XCom í‚¤ ë° íƒœìŠ¤í¬ ID ìˆ˜ì •
        aws_conn_id='minio',
        poke_interval=30, 
        timeout=600, 
        soft_fail=False, # Parquet íŒŒì¼ì´ ë°˜ë“œì‹œ ìˆì–´ì•¼ Iceberg ì‘ì—…ì´ ì˜ë¯¸ ìˆìœ¼ë¯€ë¡œ Falseë¡œ ë³€ê²½ ê³ ë ¤
    )

    # Spark ì‘ì—…: MinIOì˜ Parquet íŒŒì¼ì„ ì½ì–´ Iceberg í…Œì´ë¸”ì„ ìƒì„±/ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.
    # ì‹¤ì œ Spark ì• í”Œë¦¬ì¼€ì´ì…˜ ('/opt/spark/data/manage_iceberg_table.py')ì€ ì´ ëª©ì ì— ë§ê²Œ ì‘ì„±ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.
    manage_iceberg_table = SparkSubmitOperator(
        task_id='manage_iceberg_table_from_parquet', # íƒœìŠ¤í¬ ID ë° ì—­í•  ë³€ê²½
        application="/opt/spark/data/userlog_iceberg_spark.py", # Iceberg ì²˜ë¦¬ìš© Spark ì•± ê²½ë¡œ (ì˜ˆì‹œ)
        conn_id='spark', 
        application_args=[
            "--source_parquet_path", f"s3a://userlog-data/{{{{ ti.xcom_pull(task_ids='kafka_consumer_avro_to_parquet_minio', key='s3_object_key') }}}}",
            "--iceberg_catalog_name", "minio_catalog",
            "--iceberg_db_name", "userlog_db",
            "--iceberg_table_name", "user_activity_logs", # ëŒ€ìƒ Iceberg í…Œì´ë¸” ì´ë¦„ (ì˜ˆì‹œ, í•„ìš”ì‹œ avro_user_activity_logs ë“±ìœ¼ë¡œ ë³€ê²½)
            "--s3_endpoint", "http://54.180.166.228:9000", # MinIO ì—”ë“œí¬ì¸íŠ¸
            "--s3_access_key", "minioadmin",       # MinIO Access Key
            "--s3_secret_key", "minioadmin"        # MinIO Secret Key
        ],
        # Iceberg ì‚¬ìš©ì„ ìœ„í•´ Sparkì— í•„ìš”í•œ JARë“¤ì„ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤.
        # ì˜ˆ: iceberg-spark-runtime, aws-java-sdk-bundle ë“±
        jars="/opt/spark/jars/hadoop-aws-3.3.1.jar,/opt/spark/jars/aws-java-sdk-bundle-1.11.901.jar,/opt/spark/jars/iceberg-spark-runtime-3.4_2.12-1.4.2.jar", # ì‹¤ì œ Iceberg JAR ê²½ë¡œë¡œ ìˆ˜ì •
    )

    check_kafka_brokers_health >> consume_avro_data_to_minio >> check_minio_file_upload >> manage_iceberg_table
